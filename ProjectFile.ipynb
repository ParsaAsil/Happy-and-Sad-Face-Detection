{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6da8e94e-a248-425e-915e-ecd0b272261a",
   "metadata": {},
   "source": [
    "#  Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4052a30-148b-4584-af5b-e5870c7913cf",
   "metadata": {},
   "source": [
    "## Creating \"TVT\" folder with train and test and validation folder and Happy and Sad folder inside them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b95188d6-58ac-4320-8cbd-3c2ca0c85f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "currentDir = os.getcwd()\n",
    "TVTFolder = os.path.join(currentDir, \"TVT\")\n",
    "\n",
    "TVT = [\"Train\", \"Validation\", \"Test\"]\n",
    "HS = [\"Happy\", \"Sad\"]\n",
    "\n",
    "for main in TVT:\n",
    "    for sub in HS:\n",
    "        os.makedirs(os.path.join(TVTFolder, main,sub), exist_ok= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3e193-8c4d-4b2a-a884-3982ba3e1bdb",
   "metadata": {},
   "source": [
    "## Split the dataset in to Train, Test and Validation folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9235a3cc-7ad5-4512-ab2a-d0cc3029a872",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split into Train/Val/Test successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "currentDir = os.getcwd()\n",
    "\n",
    "trainRatio = 0.7\n",
    "valRatio = 0.15\n",
    "testRatio = 0.15\n",
    "\n",
    "HS = [\"Happy\", \"Sad\"]\n",
    "\n",
    "for file in HS:\n",
    "    folderDir = os.path.join(currentDir, \"data\", file)\n",
    "    images = os.listdir(folderDir)\n",
    "\n",
    "    random.shuffle(images)\n",
    "\n",
    "    imagesLen = len(images)\n",
    "    nTrain = int(imagesLen * trainRatio)\n",
    "    nVal = int(imagesLen * valRatio)\n",
    "\n",
    "    # Split\n",
    "    trainFiles = images[:nTrain]\n",
    "    valFiles = images[nTrain:nTrain + nVal]\n",
    "    testFiles = images[nTrain + nVal:]\n",
    "\n",
    "    # Copy to Train\n",
    "    for fname in trainFiles:\n",
    "        shutil.copy(os.path.join(folderDir, fname),\n",
    "                    os.path.join(\"TVT\", \"Train\", file, fname))\n",
    "\n",
    "    # Copy to Val\n",
    "    for fname in valFiles:\n",
    "        shutil.copy(os.path.join(folderDir, fname),\n",
    "                    os.path.join(\"TVT\", \"Validation\", file, fname))\n",
    "\n",
    "    # Copy to Test\n",
    "    for fname in testFiles:\n",
    "        shutil.copy(os.path.join(folderDir, fname),\n",
    "                    os.path.join(\"TVT\", \"Test\", file, fname))\n",
    "\n",
    "print(\"Dataset split into Train/Val/Test successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23810476-53bc-4f02-8f54-c0fbdffa1ab4",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c7dd25-6821-4f48-adf5-b1953903bc8e",
   "metadata": {},
   "source": [
    "## For handeling ICC profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e13983e9-baa2-4695-9c08-a167fe721f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parsaasil/miniforge3/envs/tf_m3/lib/python3.10/site-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "currentDir = os.getcwd()\n",
    "HS = [\"Happy\", \"Sad\"]\n",
    "TVT = [\"Train\", \"Validation\", \"Test\"]\n",
    "valid_extensions = (\".png\", \".jpg\", \".jpeg\")\n",
    "\n",
    "for TVTfolders in TVT:\n",
    "    for category in HS:\n",
    "        folderDir = os.path.join(currentDir, \"TVT\", TVTfolders, category)\n",
    "\n",
    "        for picture in os.listdir(folderDir):\n",
    "            if not picture.lower().endswith(valid_extensions):\n",
    "                continue  # skip non-image files\n",
    "\n",
    "            picturePath = os.path.join(folderDir, picture)\n",
    "\n",
    "            try:\n",
    "                img = Image.open(picturePath)\n",
    "                \n",
    "                if img.mode != 'RGB':\n",
    "                    img = img.convert('RGB')\n",
    "\n",
    "                img.save(picturePath)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {picturePath}: {e}\")\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1911ba-b347-4b1c-a60f-3f83cda898aa",
   "metadata": {},
   "source": [
    "## Changing names that start with sad or happy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "518e52e1-1013-4c4c-9a9e-7af635c72e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "currentDir = os.getcwd()\n",
    "\n",
    "HS = [\"Happy\", \"Sad\"]\n",
    "TVT = [\"Train\", \"Validation\", \"Test\"]\n",
    "\n",
    "fileNumber = 0\n",
    "\n",
    "for TVTfolder in TVT:\n",
    "    for folder in HS:\n",
    "        FolderDir = os.path.join(currentDir, \"TVT\",TVTfolder, folder)\n",
    "    \n",
    "          \n",
    "    \n",
    "        for picture in os.listdir(FolderDir):\n",
    "            fileName, fileType = os.path.splitext(picture)\n",
    "    \n",
    "            newName = f\"{folder}_{fileNumber}{fileType}\"\n",
    "            os.rename(\n",
    "                os.path.join(FolderDir, picture),\n",
    "                os.path.join(FolderDir, newName)\n",
    "            )\n",
    "    \n",
    "            \n",
    "    \n",
    "            fileNumber += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1418b8b2-6a84-49dc-9256-65e799bfc12a",
   "metadata": {},
   "source": [
    "## Crop Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1183ea3-abd9-4bca-b10a-6b9c3e04759e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    }
   ],
   "source": [
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "detector = MTCNN()\n",
    "currentDir = os.getcwd()\n",
    "HS = [\"Happy\", \"Sad\"]\n",
    "TVT = [\"Train\", \"Validation\", \"Test\"]\n",
    "\n",
    "for split in TVT:\n",
    "    for label in HS:\n",
    "        folderDir = os.path.join(currentDir, \"TVT\", split, label)\n",
    "        for img_name in os.listdir(folderDir):\n",
    "            img_path = os.path.join(folderDir, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "\n",
    "            results = detector.detect_faces(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            if results:\n",
    "                x, y, w, h = results[0]['box']\n",
    "                face_img = img[y:y+h, x:x+w]\n",
    "                cv2.imwrite(img_path, face_img)  # Overwrite or save elsewhere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b19e575-7d0d-48d5-ba39-0c7642d347e8",
   "metadata": {},
   "source": [
    "## Resize, Normalize, data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fc04a3-69f7-41ad-8ef8-9b29474087fc",
   "metadata": {},
   "source": [
    "### Resize, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7573e8ce-e1a4-41e0-9d71-c3bccdb12022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (1988, 224, 224, 3)\n",
      "Labels shape: (1988,)\n",
      "Splits shape: (1988,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "currentDir = os.getcwd()\n",
    "HS = [\"Happy\", \"Sad\"]\n",
    "TVT = [\"Train\", \"Validation\", \"Test\"]\n",
    "validExtensions = (\".png\", \".jpg\", \".jpeg\")\n",
    "\n",
    "TVTFolderDir = os.path.join(currentDir, \"TVT\")\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "splits = []  # To keep track of whether it's Train/Validation/Test\n",
    "\n",
    "for split in TVT:  # Train, Validation, Test\n",
    "    splitDir = os.path.join(TVTFolderDir, split)\n",
    "\n",
    "    for idx, dataFolder in enumerate(HS):  # Happy, Sad\n",
    "        dataFolderDir = os.path.join(splitDir, dataFolder)\n",
    "\n",
    "        if not os.path.exists(dataFolderDir):\n",
    "            print(f\"Skipping missing folder: {dataFolderDir}\")\n",
    "            continue\n",
    "\n",
    "        for picture in os.listdir(dataFolderDir):\n",
    "            _, fileType = os.path.splitext(picture)\n",
    "\n",
    "            if fileType.lower() in validExtensions:\n",
    "                pictureDir = os.path.join(dataFolderDir, picture)\n",
    "                img = cv2.imread(pictureDir)\n",
    "\n",
    "                if img is None:\n",
    "                    print(f\"Failed to load {pictureDir}\")\n",
    "                    continue\n",
    "\n",
    "                # Resize\n",
    "                img = cv2.resize(img, (224, 224))\n",
    "\n",
    "                # Normalize\n",
    "                img = img.astype(np.float32) / 255.0\n",
    "\n",
    "                # Store\n",
    "                images.append(img)\n",
    "                labels.append(idx)   # 0 for Happy, 1 for Sad\n",
    "                splits.append(split) # Train / Validation / Test\n",
    "\n",
    "# Convert to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "splits = np.array(splits)\n",
    "\n",
    "print(\"Images shape:\", images.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "print(\"Splits shape:\", splits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a7952-0f96-4ef9-bb09-c242ebc718c6",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f34c16e-a530-41fc-a115-43c2e3a897e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented batch shape: (16, 224, 224, 3)\n",
      "Augmented labels shape: (16,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "# Select only training data\n",
    "train_images = images[splits == \"Train\"]\n",
    "train_labels = labels[splits == \"Train\"]\n",
    "\n",
    "# Optionally, validation data (no augmentation)\n",
    "val_images = images[splits == \"Validation\"]\n",
    "val_labels = labels[splits == \"Validation\"]\n",
    "\n",
    "# Create the data augmentation generator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Fit the generator (optional, only needed for featurewise normalization)\n",
    "train_datagen.fit(train_images)\n",
    "\n",
    "# Create iterator\n",
    "batch_size = 16\n",
    "train_generator = train_datagen.flow(train_images, train_labels, batch_size=batch_size)\n",
    "val_generator = ImageDataGenerator().flow(val_images, val_labels, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Example: get one batch\n",
    "augmented_images, augmented_labels = next(train_generator)\n",
    "print(\"Augmented batch shape:\", augmented_images.shape)\n",
    "print(\"Augmented labels shape:\", augmented_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6334a0da-e5cf-4696-8a19-08d0c6f375c8",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09d7fbb4-376b-43ed-be4d-024c7aea7a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 271ms/step - accuracy: 0.5651 - loss: 1.1706 - val_accuracy: 0.7239 - val_loss: 0.5537\n",
      "Epoch 2/20\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 330ms/step - accuracy: 0.6211 - loss: 1.0950 - val_accuracy: 0.7475 - val_loss: 0.5451\n",
      "Epoch 3/20\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 331ms/step - accuracy: 0.6736 - loss: 0.9854 - val_accuracy: 0.7677 - val_loss: 0.4955\n",
      "Epoch 4/20\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 302ms/step - accuracy: 0.6592 - loss: 1.0281 - val_accuracy: 0.7879 - val_loss: 0.4931\n",
      "Epoch 5/20\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 281ms/step - accuracy: 0.6556 - loss: 0.9699 - val_accuracy: 0.7912 - val_loss: 0.4975\n",
      "Epoch 6/20\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 284ms/step - accuracy: 0.6887 - loss: 0.9334 - val_accuracy: 0.7710 - val_loss: 0.5575\n",
      "Epoch 7/20\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 301ms/step - accuracy: 0.7009 - loss: 0.8845 - val_accuracy: 0.7946 - val_loss: 0.5019\n",
      "Epoch 8/20\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 305ms/step - accuracy: 0.6952 - loss: 0.9861 - val_accuracy: 0.7879 - val_loss: 0.5285\n",
      "Epoch 9/20\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 305ms/step - accuracy: 0.7168 - loss: 0.8795 - val_accuracy: 0.7912 - val_loss: 0.4936\n",
      "Epoch 10/20\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 295ms/step - accuracy: 0.7038 - loss: 0.9698 - val_accuracy: 0.7845 - val_loss: 0.5991\n",
      "Epoch 11/20\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 306ms/step - accuracy: 0.7045 - loss: 0.9011 - val_accuracy: 0.7912 - val_loss: 0.5190\n",
      "Epoch 12/20\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 306ms/step - accuracy: 0.7232 - loss: 0.8881 - val_accuracy: 0.8081 - val_loss: 0.5068\n",
      "Epoch 13/20\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 298ms/step - accuracy: 0.7304 - loss: 0.8872 - val_accuracy: 0.7879 - val_loss: 0.6627\n",
      "Epoch 14/20\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 301ms/step - accuracy: 0.7175 - loss: 0.9169 - val_accuracy: 0.8114 - val_loss: 0.5294\n",
      "Epoch 15/20\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 307ms/step - accuracy: 0.7405 - loss: 0.9064 - val_accuracy: 0.7845 - val_loss: 0.7265\n",
      "Epoch 16/20\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 311ms/step - accuracy: 0.7390 - loss: 0.8571 - val_accuracy: 0.8114 - val_loss: 0.5688\n",
      "Epoch 17/20\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 299ms/step - accuracy: 0.7635 - loss: 0.8155 - val_accuracy: 0.8249 - val_loss: 0.5365\n",
      "Epoch 18/20\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 313ms/step - accuracy: 0.7477 - loss: 0.7754 - val_accuracy: 0.8114 - val_loss: 0.5210\n",
      "Epoch 19/20\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 311ms/step - accuracy: 0.7628 - loss: 0.7022 - val_accuracy: 0.8215 - val_loss: 0.5435\n",
      "Epoch 20/20\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 303ms/step - accuracy: 0.7656 - loss: 0.7537 - val_accuracy: 0.8350 - val_loss: 0.5282\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load VGG16 without top layers (we'll add our own classifier)\n",
    "base_model = VGG16(weights='models/vgg16_notop.h5', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze base model layers so we don't train them yet\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom classifier on top\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')  # 2 classes: Happy, Sad\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='sparse_categorical_crossentropy',  # because labels are integers\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train using your data generator\n",
    "batch_size = 16\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    #steps_per_epoch=len(train_images)//batch_size,\n",
    "    validation_data=val_generator,\n",
    "    #validation_steps=len(val_images)//batch_size,\n",
    "    epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035ea806-a195-400c-9191-3a5f9f42f247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model\n",
    "model.save(\"vgg16_emotion_model.h5\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f088ae41-ea86-4962-af42-be521041ac2c",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27efff53-7847-4ade-8251-f07c369d0e87",
   "metadata": {},
   "source": [
    "## Testing all \"Test\" folder pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ee0da-7dfb-4b82-ba99-4ea140f21957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 09:38:49.099973: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3\n",
      "2025-09-03 09:38:49.099998: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-09-03 09:38:49.100003: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-09-03 09:38:49.100177: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-09-03 09:38:49.100185: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-09-03 09:39:05.167124: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 304ms/step\n",
      "Test Accuracy: 0.8253968253968254\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "currentDir = os.getcwd()\n",
    "test_dir = os.path.join(currentDir, \"TVT\", \"Test\")\n",
    "\n",
    "\n",
    "# Load model\n",
    "model = load_model(\"vgg16_emotion_model.h5\")  # replace with your model path\n",
    "\n",
    "# Test folder path\n",
    "classes = [\"Happy\", \"Sad\"]\n",
    "\n",
    "detector = MTCNN()\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for idx, cls in enumerate(classes):\n",
    "    folder = os.path.join(test_dir, cls)\n",
    "    for img_name in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        \n",
    "        # Detect face\n",
    "        results = detector.detect_faces(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        if results:\n",
    "            x, y, w, h = results[0]['box']\n",
    "            x, y = max(0, x), max(0, y)  # avoid negative values\n",
    "            face_img = img[y:y+h, x:x+w]\n",
    "            face_img = cv2.resize(face_img, (224, 224))\n",
    "            face_img = face_img.astype(np.float32) / 255.0\n",
    "            X_test.append(face_img)\n",
    "            y_test.append(idx)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Predict\n",
    "preds = model.predict(X_test)\n",
    "pred_classes = np.argmax(preds, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.sum(pred_classes == y_test) / len(y_test)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b7cdaa-0eda-440e-9688-caff461216c7",
   "metadata": {},
   "source": [
    "## Testing specific picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5a53dd-b8fe-4f13-89e9-f87fdb0842ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "Predicted: Happy\n"
     ]
    }
   ],
   "source": [
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "detector = MTCNN()\n",
    "currentDir = os.getcwd()\n",
    "\n",
    "model = load_model(\"vgg16_emotion_model2.h5\")  # replace with your file path\n",
    "img_name = \"333.jpg\"  # put your image name here\n",
    "folderDir = os.path.join(currentDir, \"TestImage\")  # corrected variable name\n",
    "img_path = os.path.join(folderDir, img_name)\n",
    "\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "if img is None:\n",
    "    print(\"Image not found!\")\n",
    "else:\n",
    "    results = detector.detect_faces(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    if results:\n",
    "        x, y, w, h = results[0]['box']\n",
    "        face_img = img[y:y+h, x:x+w]\n",
    "\n",
    "        face_img = cv2.resize(face_img, (224, 224))\n",
    "\n",
    "        # Normalize\n",
    "        face_img = face_img.astype(np.float32) / 255.0\n",
    "\n",
    "        # Add batch dimension\n",
    "        face_img = np.expand_dims(face_img, axis=0)  # shape becomes (1, 224, 224, 3)\n",
    "        \n",
    "        # Predict\n",
    "        pred = model.predict(face_img)\n",
    "        pred_class = np.argmax(pred, axis=1)[0]\n",
    "        \n",
    "        # Map to labels\n",
    "        labels = [\"Happy\", \"Sad\"]\n",
    "        print(\"Predicted:\", labels[pred_class])\n",
    "\n",
    "        \n",
    "    else:\n",
    "        print(\"No face detected in the image.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
